# ğŸ”— Recommender System Prediction with Dynamic Graph Representation Learning

> **Link prediction in evolving graphs using GNNs and RNNs â€“ enhanced with Node2Vec and robust to noisy data.**



## Abstract
This project tackles link prediction in **dynamic graphs**, a crucial task in recommender systems and fraud detection. Unlike static graphs, dynamic graphs evolve over time, capturing real-world complexity more effectively.

We address two key challenges:

1. â„ï¸ **Cold Start Problem** â€“ Predicting links for unseen nodes.
2. ğŸ”¨ **Noise Robustness** â€“ Measuring how random edge additions affect performance.

Our approach is based on the **Euler model**: a state-of-the-art hybrid architecture combining Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) for topological and temporal learning.

---

## ğŸš€ Features
- Link prediction on **temporal** and **topological** patterns.
- **Cold Start node handling** via Node2Vec (replacement or concatenation).
- Noise injection (1â€“10%) to test robustness.
- Evaluation on 3 datasets: `Enron10`, `DBLP`, and `Facebook`.

---

## ğŸ“ Model Architecture

| Component | Role |
|----------|------|
| `GCN` Encoder | Learns graph structure embeddings |
| `RNN` Encoder | Captures temporal dynamics |
| `Dot Product Decoder` | Predicts edge existence |
| `Binary Cross Entropy` | Optimizes link prediction |

---

## ğŸ’» Implementation Highlights

### Euler Model Enhancements
- **Node2Vec** for new nodes (2 strategies: replace vs. concat)
- Swappable RNN cells (`RNN`, `GRU`, `LSTM`)
- Full config support: learning rate, batch size, embedding dims...

### Node Embedding Strategies
| Strategy | Description |
|---------|-------------|
| **Replacement** | Keep original embedding & use Node2Vec only for new nodes representations|
| **Concatenation** | Combine original & Node2Vec embedding by concatenation |

ğŸ” *Concatenation yields better results on `DBLP` and `Facebook` datasets.*

---

## ğŸ“„ Node2Vec Training Example

```python
model_n2v = Node2Vec(
    edges, embedding_dim=32, walk_length=50,
    context_size=25, walks_per_node=25, 
    num_negative_samples=1, sparse=True
)

optimizer = torch.optim.SparseAdam(model_n2v.parameters(), lr=0.01)

for epoch in range(150):
    model_n2v.train()
    for pos_rw, neg_rw in model_n2v.loader(batch_size=128):
        optimizer.zero_grad()
        loss = model_n2v.loss(pos_rw, neg_rw)
        loss.backward()
        optimizer.step()
```

Embeddings are saved per graph snapshot and reused during training.

---

## âš™ï¸ Experimental Setup

| Parameter | Value |
|----------|-------|
| ğŸ“ Datasets | `Enron10`, `DBLP`, `Facebook` |
| ğŸ“ Metrics | `AUC`, `Average Precision (AP)` |
| âš–ï¸ Baseline | Original Euler model |
| ğŸ’¥ Noise Levels | 0%, 1%, 5%, 10% |

---

## ğŸ“ˆ Key Results

- **Cold Start**: Node2Vec concatenation significantly improves performance for unseen nodes.
- **Noise Sensitivity**: Most models degrade with noise, but DBLP shows resilience (less overfitting).
- **Architecture Flexibility**: Swappable RNNs and embedding fusion yield consistent gains.

---

## ğŸ“„ Full Report & Visuals

ğŸ‘‰ **[ğŸ“˜ Read the Full Report (PDF)](https://github.com/user-attachments/files/18669799/recommander.systems.1.pdf)**

![Preview](https://github.com/user-attachments/assets/9b65e15a-a895-468b-80b5-c6f50df0ad42)

---

## ğŸ“š References

> King, I. J., & Huang, H. H. (2023).  
> *Euler: Detecting network lateral movement via scalable temporal link prediction*.  
> _ACM Transactions on Privacy and Security_.

---

## ğŸ§‘â€ğŸ’» Author

**Amine Mohabeddine** â€“ [GitHub](https://github.com/AmineM89) | [LinkedIn](https://linkedin.com/in/AmineM89)
